# -*- coding: utf-8 -*-
"""HeartDisease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jZPUTZy0tCzRc9_ToF6d0dAPsNvHMM7I

# **Predicting Heart Disease Risk: A CRISP-DM Framework with a Streamlit Web Application**
"""



import pandas as pd
import io
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
import sklearn.metrics as metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
import shap
import streamlit as st

# Import necessary libraries for EDA
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st # Using Streamlit for potential app-like presentation

# Load the dataset
@st.cache_data # Streamlit decorator to cache data loading
def load_data(file_path):
    df = pd.read_csv(file_path)
    return df

df = load_data('heart.csv')

# --- Phase 2: Data Understanding (EDA from phase2.py) ---
st.title('Data Understanding')
st.subheader('First Few Rows of Data')
st.dataframe(df.head())

st.subheader('Data Description')
st.dataframe(df.describe())

st.subheader('Histograms of All Features')
df.hist(bins=20, figsize=(15, 10), edgecolor='black')
plt.suptitle('Histograms of All Features', fontsize=16)
fig1 = plt.gcf()
st.pyplot(fig1)
plt.clf()

st.subheader('Boxplots of All Features')
plt.figure(figsize=(15, 10))
for i, column in enumerate(df.columns[:-1], 1):  # Skip 'target' or last col if needed
    plt.subplot(4, 4, i)
    sns.boxplot(y=df[column])
    plt.title(f'Boxplot of {column}')
plt.tight_layout()
fig2 = plt.gcf()
st.pyplot(fig2)
plt.clf()

st.subheader('Correlation Matrix Heatmap')
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix Heatmap')
fig3 = plt.gcf()
st.pyplot(fig3)
plt.clf()

st.subheader('Scatterplots: Age vs. Cholesterol, Max Heart Rate, ST Depression')
plt.figure(figsize=(18, 5))
plt.subplot(1, 3, 1)
sns.scatterplot(x='age', y='chol', hue='target', data=df)
plt.title('Age vs. Cholesterol')
plt.subplot(1, 3, 2)
sns.scatterplot(x='age', y='thalach', hue='target', data=df)
plt.title('Age vs. Max Heart Rate')
plt.subplot(1, 3, 3)
sns.scatterplot(x='age', y='oldpeak', hue='target', data=df)
plt.title('Age vs. ST Depression')
plt.tight_layout()
fig4 = plt.gcf()
st.pyplot(fig4)
plt.clf()

st.subheader('Missing Values in the Dataset')
st.write(df.isnull().sum())

st.subheader('Outlier Summary (IQR Method)')
outliers_summary = {}
for column in df.select_dtypes(include=['float64', 'int64']).columns:
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    outliers_summary[column] = len(outliers)
st.write(outliers_summary)

"""## **Data Preparation**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer # Though not used as no missing values
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
# It's assumed the 'heart.csv' file is in the same directory or a known path.
file_path = "heart.csv"
df = pd.read_csv(file_path)

# Display the first few rows to recall the data structure
print("Original DataFrame head:")
print(df.head())
print("\nOriginal DataFrame info:")
df.info()

"""Following the data understanding phase, the data preparation phase focuses on transforming the selected raw data into a clean, well-structured format suitable for the chosen modeling techniques. This involves several key steps outlined below, specifically applied to the 'heart.csv' dataset.

## **Select Data**
"""

# Display basic information about the dataset to re-verify columns
if not df.empty:
    print("Dataset Information:")
    df.info()
    print("\nDataset Columns:")
    print(df.columns)

    # Assuming 'target' is the name of the outcome variable.
    # If your target variable has a different name, please adjust it here.
    target_variable = 'target'

    if target_variable not in df.columns:
        print(f"\nError: Target variable '{target_variable}' not found in the dataset columns.")
        print("Please ensure the target variable name is correct.")
        # For demonstration, we'll list available columns if 'target' is missing
        # and you might need to manually set features and target.
        features = pd.DataFrame() # Empty
        target = pd.Series(dtype='float64') # Empty
    else:
        # All columns except the target variable are considered features.
        features = df.drop(columns=[target_variable])
        target = df[target_variable]
        print(f"\nSelected features (X) - first 5 rows:")
        print(features.head())
        print(f"\nSelected target variable (y) - first 5 rows:")
        print(target.head())
        print(f"\nAll {len(features.columns)} available features have been selected for initial analysis.")
else:
    print("DataFrame is empty. Cannot select data.")

"""All features present in this standard heart disease dataset (e.g., age, sex, cholesterol, blood pressure, etc.) are known to be clinically relevant or potentially correlated with heart conditions. Therefore, retaining all of them for the initial modeling stages is a common practice. Feature selection techniques can be applied later, after initial model evaluation, if dimensionality reduction is needed or if some features prove to be non-informative. The 'target' column directly indicates the presence or absence of heart disease, making it the clear dependent variable.

## **Clean Data**

Data cleaning involves addressing inconsistencies and errors identified in the Data Understanding phase. This typically includes handling missing values, correcting errors, and dealing with outliers. For this dataset, the process first checks for missing values. If any are found, a common strategy for numerical features is imputation (e.g., using the mean or median), and for categorical features, imputation with the mode or a constant. Outlier detection might have been performed visually (e.g., box plots) or statistically in the previous phase. For this example, it's assumed that significant outliers that distort the dataset might be capped or transformed, but often, tree-based models are robust to outliers, so aggressive removal isn't always necessary. Here, the focus will be on handling missing values.
"""

# Verify missing values again
print("\nMissing values in the dataset:")
print(df.isnull().sum())

numerical_cols_for_outliers = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols_for_outliers):
    plt.subplot(2, 3, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Box plot of {col}')
plt.tight_layout()
plt.show()
# The plot will be generated by the python interpreter and shown in its output.
# In a Streamlit app, you would use st.pyplot(fig)

"""## **Construct Data**

This step involves deriving new attributes from existing ones or transforming variables to make them more suitable for modeling. Feature construction can help uncover more complex relationships in the data. Common tasks include creating interaction terms, polynomial features, or binning numerical data. One of the most critical tasks in this step for many datasets is encoding categorical variables into a numerical format, as most machine learning algorithms require numerical input. For this dataset, several columns are categorical (e.g., 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'). These will be converted into numerical representations using techniques like one-hot encoding.
"""

if not features.empty:
    print("Original features before constructing data:")
    print(features.head())
    print(f"\nData types before encoding:\n{features.dtypes}")

    # Identify categorical features that need encoding.
    # Based on common knowledge of this dataset, these columns are often categorical.
    # The exact list might vary based on the specific version of 'heart.csv'.
    # It's important to have identified these during Data Understanding.

    # Let's inspect unique values to confirm categorical nature for potential candidates
    potential_cat_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
    categorical_features_to_encode = []
    for col in potential_cat_cols:
        if col in features.columns:
            if features[col].nunique() < 10 or features[col].dtype == 'object': # Heuristic for categorical
                categorical_features_to_encode.append(col)

    print(f"\nIdentified categorical features for encoding: {categorical_features_to_encode}")

    if not categorical_features_to_encode:
        print("\nNo categorical features identified for encoding, or they might already be numeric.")
        print("If you have categorical features represented as numbers with implied order, consider their nature.")
        features_constructed = features.copy()
    else:
        # Using One-Hot Encoding for nominal categorical features
        # One-hot encoding creates new columns for each category, avoiding implied ordinal relationships.
        # `drop='first'` can be used to avoid multicollinearity if needed, but not always necessary
        # and depends on the model being used. For simplicity, we'll include all dummies here.
        features_constructed = pd.get_dummies(features, columns=categorical_features_to_encode, drop_first=False)
        print("\nFeatures after one-hot encoding (first 5 rows):")
        print(features_constructed.head())
        print(f"\nShape of features before encoding: {features.shape}")
        print(f"Shape of features after encoding: {features_constructed.shape}")

    # Example of creating a new feature (if relevant):
    # For instance, one might create an age group category, or a cholesterol/age ratio.
    # For this example, we'll stick to encoding as the primary construction task.
    # df['age_group'] = pd.cut(df['age'], bins=[0, 40, 60, 100], labels=['Young', 'Middle-aged', 'Senior'])
    print("\nNo other new features were explicitly constructed in this example, beyond encoding.")
else:
    print("Features DataFrame is empty. Cannot construct data.")
    features_constructed = pd.DataFrame()

"""The columns 'ca' (number of major vessels colored by flouroscopy) and 'thal' (thalassemia type) are sometimes loaded as numerical but represent discrete categories. If pd.get_dummies is applied to numerical columns that are truly categorical, it will create dummies for each numerical value. Ensure these were correctly identified as categorical or converted to an object/category type if necessary before get_dummies. The heuristic nunique() < 10 helps catch some of these.

### Other Codes
"""

# Identify categorical and numerical features
categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

# Verifying unique values in categorical columns to understand their nature
print("\nUnique values in identified categorical columns:")
for col in categorical_cols:
    print(f"{col}: {df[col].unique()}")

# The 'target' column is also binary but is the dependent variable.
# X will be our feature set for transformations.
X = df.drop('target', axis=1)
y = df['target']

"""## **Integrate Data**

The data integration step is relevant when data is collected from multiple sources, such as different databases, files, or APIs. The goal is to combine these disparate datasets into a single, cohesive dataset for analysis. This might involve merging tables based on common keys or appending datasets with similar structures. For this specific project, it is assumed that all necessary data is contained within the single heart.csv file. Therefore, no complex data integration steps are required.
"""

print("Data Integration Step:")
print("This project utilizes a single data source ('heart.csv').")
print("Therefore, no specific data integration steps (like merging multiple tables) are required.")
# The 'features_constructed' DataFrame is the current working dataset.

"""## **Format Data**

Data formatting ensures that the data is in the precise structure required by the machine learning algorithms. This often involves scaling numerical features. Many algorithms, such as Support Vector Machines (SVMs), K-Nearest Neighbors (KNN), and neural networks, are sensitive to the scale of input features. Features with larger values might dominate the learning process. Standardization (scaling to zero mean and unit variance) or Normalization (scaling to a specific range, typically [0, 1]) are common techniques. Here, numerical features (those not created by one-hot encoding and originally numerical) will be scaled using StandardScaler.
"""

if not features_constructed.empty:
    print("Formatting Data: Scaling numerical features.")

    # Identify numerical columns that were not created by one-hot encoding (i.e., original numerical features)
    # One-hot encoded columns are already binary (0 or 1) and typically do not need scaling.

    # Get list of columns that were originally numerical and not part of the one-hot encoded set
    original_numerical_cols = [col for col in features.select_dtypes(include=np.number).columns
                               if col not in categorical_features_to_encode]

    print(f"\nOriginal numerical columns to be scaled: {original_numerical_cols}")

    if not original_numerical_cols:
        print("No numerical features to scale (perhaps all were categorical or already scaled).")
        features_formatted = features_constructed.copy()
    else:
        # Apply StandardScaler to these numerical columns
        scaler = StandardScaler()

        # Create a copy to avoid SettingWithCopyWarning if features_constructed is a slice
        features_formatted = features_constructed.copy()

        features_formatted[original_numerical_cols] = scaler.fit_transform(features_constructed[original_numerical_cols])

        print("\nFeatures after scaling (first 5 rows):")
        print(features_formatted.head())
        print("\nDescriptive statistics of scaled numerical features (should show mean ~0 and std ~1):")
        print(features_formatted[original_numerical_cols].describe())
else:
    print("Constructed features DataFrame is empty. Cannot format data.")
    features_formatted = pd.DataFrame()

"""### Other Codes

Formatting data ensures it's in a structure that is suitable for machine learning algorithms. This typically involves:

Encoding Categorical Features: Converting categorical data into a numerical
format that models can understand (e.g., one-hot encoding).

Scaling Numerical Features: Standardizing or normalizing numerical features so that they have a similar range of values. This is important for algorithms that are sensitive to feature scales (e.g., SVM, Logistic Regression, k-NN, Neural Networks).

A ColumnTransformer from scikit-learn is an excellent tool for applying different transformations to different columns.
"""

# Define the transformers
# For numerical columns: Standard Scaling
numerical_transformer = StandardScaler()

# For categorical columns: One-Hot Encoding
# handle_unknown='ignore' helps if test set has categories not seen in train
categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

# Create a preprocessor object using ColumnTransformer
# This applies the specified transformers to the specified columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ],
    remainder='passthrough' #  Keeps any columns not specified (should be none here if X is defined correctly)
)

# Apply the transformations to the feature set X
# Note: fit_transform is used on the entire X for now.
# Ideally, fit_transform on training data and transform on test data.
# This will be correctly handled when we split data in the next step.
# For demonstration of how X_processed looks:
X_processed_demonstration = preprocessor.fit_transform(X)

# Get feature names after one-hot encoding for better understanding
# This can be a bit tricky with ColumnTransformer directly, but we can infer or use get_feature_names_out
# (available in newer scikit-learn versions)
try:
    feature_names_out = preprocessor.get_feature_names_out()
except AttributeError: # Older scikit-learn
    # Manually construct feature names for demonstration
    # For numerical features, names remain the same
    ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)
    feature_names_out = numerical_cols + list(ohe_feature_names)


X_processed_df_demonstration = pd.DataFrame(X_processed_demonstration, columns=feature_names_out)

print("\nShape of processed X for demonstration:", X_processed_df_demonstration.shape)
print("Head of processed X (DataFrame) for demonstration:")
print(X_processed_df_demonstration.head())
print("\nNote: The actual fitting of the preprocessor should happen only on the training data.")

"""The preprocessor is now set up. Numerical features will be scaled, and categorical features will be one-hot encoded. The fit_transform method will be applied to the training data, and transform will be applied to the test data to prevent data leakage. The demonstration above shows the structure of the processed data.

## **Split Data**

The final step in data preparation is to split the dataset into training and testing sets. The training set is used to train the machine learning model, while the testing set is used to evaluate its performance on unseen data. This helps to assess how well the model generalizes to new, independent data. A common split ratio is 70-80% for training and 20-30% for testing. It's also good practice to use stratify=y if it's a classification task, especially with imbalanced classes, to ensure that both training and testing sets have a similar proportion of target classes. A random_state is set for reproducibility.

## **Code #1**
"""

if not features_formatted.empty and not target.empty:
    # Splitting the data into training and testing sets
    # Using a common 80/20 split.
    # 'stratify=target' is important for classification tasks to ensure proportional class representation.
    # 'random_state' ensures reproducibility of the split.
    X_train, X_test, y_train, y_test = train_test_split(
        features_formatted,
        target,
        test_size=0.2,       # 20% for testing
        random_state=42,     # For reproducibility
        stratify=target      # Ensure similar class proportions in train and test sets
    )

    st.subheader('Data Splitting Results (Code #1)')
    st.write('Data splitting complete.')
    st.write(f"Shape of X_train: {X_train.shape}")
    st.write(f"Shape of X_test: {X_test.shape}")
    st.write(f"Shape of y_train: {y_train.shape}")
    st.write(f"Shape of y_test: {y_test.shape}")

    st.write('Proportion of target variable in original dataset:')
    st.write(target.value_counts(normalize=True))
    st.write('Proportion of target variable in y_train:')
    st.write(y_train.value_counts(normalize=True))
    st.write('Proportion of target variable in y_test:')
    st.write(y_test.value_counts(normalize=True))

    st.write('First 5 rows of X_train:')
    st.dataframe(X_train.head())

else:
    st.warning('Formatted features or target DataFrame is empty. Cannot split data.')
    # Define empty DataFrames/Series if splitting failed, to avoid errors in subsequent (hypothetical) cells
    X_train, X_test, y_train, y_test = pd.DataFrame(), pd.DataFrame(), pd.Series(dtype='float64'), pd.Series(dtype='float64')

"""## **Code #2**"""

# Splitting the data into training and testing sets
# X contains the original features, y contains the target variable
# The preprocessor defined earlier will be fit on X_train and used to transform X_train and X_test.

X_train_orig, X_test_orig, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 20% for testing
    random_state=42,    # For reproducibility
    stratify=y          # Ensure proportional class representation
)

st.subheader('Data Splitting Results (Code #2)')
st.write(f"Shape of original training features (X_train_orig): {X_train_orig.shape}")
st.write(f"Shape of original test features (X_test_orig): {X_test_orig.shape}")
st.write(f"Shape of training target (y_train): {y_train.shape}")
st.write(f"Shape of test target (y_test): {y_test.shape}")
st.write('Class distribution in y_train:')
st.write(y_train.value_counts(normalize=True))
st.write('Class distribution in y_test:')
st.write(y_test.value_counts(normalize=True))

# Now, apply the preprocessor
# Fit the preprocessor on the training data and transform both training and test data
X_train_processed = preprocessor.fit_transform(X_train_orig)
X_test_processed = preprocessor.transform(X_test_orig)

# Get the feature names after transformation for the processed dataframes
try:
    # For scikit-learn 1.0+
    processed_feature_names = preprocessor.get_feature_names_out()
except AttributeError:
    # For older scikit-learn versions, manual construction might be needed
    # Or access through named_transformers_ as done in the demonstration
    ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)
    processed_feature_names = numerical_cols + list(ohe_feature_names)

# Convert processed arrays back to DataFrames (optional, but good for inspection)
X_train = pd.DataFrame(X_train_processed, columns=processed_feature_names, index=X_train_orig.index)
X_test = pd.DataFrame(X_test_processed, columns=processed_feature_names, index=X_test_orig.index)

st.write(f"Shape of processed training features (X_train): {X_train.shape}")
st.write('Head of processed training features (X_train):')
st.dataframe(X_train.head())
st.write(f"Shape of processed test features (X_test): {X_test.shape}")
st.write('Head of processed test features (X_test):')
st.dataframe(X_test.head())

"""## **Saving a csv file for data modeling**"""

# Save the processed data (optional)
X_train.to_csv('train_features.csv', index=False)
X_test.to_csv('test_features.csv', index=False)
y_train.to_csv('train_target.csv', index=False, header=['target'])
y_test.to_csv('test_target.csv', index=False, header=['target'])

print("\nProcessed dataframes (X_train, X_test, y_train, y_test) are ready.")
print("Consider saving them to CSV if you want to use them directly in the next phase without rerunning this script.")

"""# **Data Modeling**

With the data prepared (ideally from a comprehensive Phase 3), the project now focuses on constructing and evaluating predictive models. This involves selecting appropriate algorithms based on the task (binary classification for heart disease ), training these models with the prepared training data, and then rigorously assessing their performance on unseen test data.

## **Data Loading and Basic Preparation**

Before modeling, the dataset provided (heart.csv ) needs to be loaded. For the modeling phase, it's assumed that extensive data preparation (as outlined in Phase 3 of CRISP-DM) has already been performed. This would typically include handling missing values, encoding categorical features, and feature scaling. For this demonstration, basic preparation steps like splitting the data into features (X) and target (y), and then into training and testing sets will be performed. Feature scaling will also be applied as it's beneficial for many algorithms

### **With Streamlit**
"""

# Load the dataset
@st.cache_data # Streamlit decorator to cache data loading
def load_data(file_path):
    df = pd.read_csv(file_path)
    return df

df = load_data('heart.csv')

# Basic data exploration (optional, assuming done in Phase 2 & 3)
st.write("Dataset Head:", df.head())
st.write("Dataset Info:", df.info())
st.write("Dataset Description:", df.describe())

# Define features (X) and target (y)
# The 'target' column is typically the variable we want to predict.
X = df.drop('target', axis=1)
y = df['target']

# Split the data into training and testing sets
# This is crucial to evaluate the model on unseen data.
# A common split is 80% for training and 20% for testing.
# random_state is set for reproducibility.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Feature Scaling
# Many machine learning algorithms perform better when numerical features are scaled.
# StandardScaler standardizes features by removing the mean and scaling to unit variance.
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert scaled arrays back to DataFrames (optional, but can be useful for SHAP)
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns)

st.write("Shape of X_train:", X_train_scaled_df.shape)
st.write("Shape of X_test:", X_test_scaled_df.shape)
st.write("Shape of y_train:", y_train.shape)
st.write("Shape of y_test:", y_test.shape)

"""In this section, the heart.csv dataset  is loaded into a pandas DataFrame. The features (independent variables) are separated from the target variable (target). The dataset is then split into training and testing sets using an 80/20 ratio, with stratify=y to ensure that the proportion of the target variable is similar in both train and test sets, which is important for classification tasks, especially with imbalanced datasets. Finally, StandardScaler is applied to standardize the feature values, which helps to improve the performance and convergence of algorithms like Logistic Regression and SVMs.

### **Without Streamlit**
"""

# Import necessary libraries for modeling and evaluation
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV, train_test_split # train_test_split for placeholder
import pandas as pd # To store results
import numpy as np # For placeholder

# --- PREPARATION FOR PHASE 4 ---
# In a real Jupyter Notebook environment, X_train, X_test, y_train, y_test
# would be available in memory from executing Phase 3.

# The following lines attempt to load 'heart.csv' and perform a very minimal
# preparation, JUST to make X_train, etc. available if Phase 3 was not run prior to this script.
# THIS IS NOT A SUBSTITUTE FOR THE COMPLETE PHASE 3.
# For a proper analysis, ensure Phase 3 is fully completed.

try:
    # Check if the crucial variables from Phase 3 exist
    X_train.shape
    print("Data (X_train, X_test, y_train, y_test) from Phase 3 is assumed to be loaded and ready.")
except NameError:
    print("Warning: X_train, X_test, y_train, y_test not found from a previous Phase 3 execution.")
    print("Attempting a basic load and split of 'heart.csv' for demonstration purposes ONLY.")
    print("For a full and proper analysis, please run the complete Phase 3: Data Preparation first.")

    try:
        # Load the dataset directly - assuming 'heart.csv' is in the same directory or use the uploaded content
        # For this example, we'll simulate reading from the provided content structure
        # In a real script, you'd use: df = pd.read_csv('heart.csv')
        # Using the column names from the provided file [cite: 1]
        column_names = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','target']
        # If running in an environment where the file content can be directly used:
        # import io
        # file_content = """age,sex,... (rest of the CSV data)""" # Replace with actual file content string
        # df = pd.read_csv(io.StringIO(file_content))

        # As a robust fallback if direct file reading is an issue in this context:
        # Try to load from the filename 'heart.csv'
        df = pd.read_csv('heart.csv') # Assumes heart.csv is accessible
        print("Successfully loaded 'heart.csv'.")

        # Very basic preprocessing (minimal example, Phase 3 does this thoroughly)
        # Define features (X) and target (y)
        if 'target' in df.columns:
            X = df.drop('target', axis=1)
            y = df['target']

            # Handle categorical features - quick one-hot encoding for any object types
            # A proper Phase 3 would be more nuanced.
            X = pd.get_dummies(X, drop_first=True)

            # Ensure all data is numeric and handle potential NaNs from get_dummies if categories were NaN
            X = X.apply(pd.to_numeric, errors='coerce').fillna(0)

            # Splitting data
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
            print(f"Demo X_train shape: {X_train.shape}, Demo y_train shape: {y_train.shape}")
            print("Basic data split complete for demonstration.")
        else:
            print("Error: 'target' column not found in the CSV. Cannot proceed with demo preparation.")
            # Initialize empty to prevent downstream errors if demo prep fails
            X_train, X_test, y_train, y_test = pd.DataFrame(), pd.DataFrame(), pd.Series(dtype='float64'), pd.Series(dtype='float64')

    except FileNotFoundError:
        print("Error: 'heart.csv' not found for demo preparation. Please ensure the file is accessible.")
        X_train, X_test, y_train, y_test = pd.DataFrame(), pd.DataFrame(), pd.Series(dtype='float64'), pd.Series(dtype='float64')
    except Exception as e:
        print(f"An error occurred during demo preparation: {e}")
        X_train, X_test, y_train, y_test = pd.DataFrame(), pd.DataFrame(), pd.Series(dtype='float64'), pd.Series(dtype='float64')

print("-" * 50)
# --- END OF PREPARATION FOR PHASE 4 ---

"""**Select Modeling Technique(s)**

The choice of modeling technique depends on the nature of the problem (classification, regression, clustering), the characteristics of the data, and the project goals. For this heart disease prediction task, the 'target' variable is binary (0 for no heart disease, 1 for heart disease), making it a binary classification problem.

Several algorithms are suitable for binary classification. The following will be explored:

*   **Logistic Regression**: A linear model that is widely used for binary classification. It's relatively simple, interpretable, and performs well on linearly separable data.
*   **Decision Trees**: A non-linear model that creates a tree-like structure of decisions. They are easy to understand and visualize but can be prone to overfitting.
*   **Random Forests**: An ensemble learning method that builds multiple decision trees and merges their predictions. It generally provides higher accuracy and is more robust against overfitting than individual decision trees.
*   **Support Vector Machines (SVM)**: A powerful model that finds an optimal hyperplane to separate classes in a high-dimensional space. SVMs can be effective in high-dimensional spaces and with clear margins of separation, but can be computationally intensive.

These models offer a good mix of complexity, interpretability, and performance characteristics.
"""

# Initialize the chosen models
models = {
    "Logistic Regression": LogisticRegression(solver='liblinear', max_iter=1000, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Support Vector Classifier": SVC(probability=True, random_state=42), # probability=True for ROC AUC
    "K-Nearest Neighbors": KNeighborsClassifier()
}

print("Selected modeling techniques for heart disease prediction:")
for model_name in models.keys():
    print(f"- {model_name}")

"""**Build Model(s)**

This step involves the actual training of the selected machine learning models. Each algorithm is fitted using the X_train (prepared features) and y_train (corresponding target labels from the heart.csv dataset ) generated in Phase 3. During this process, the models learn to identify patterns and relationships within the training data that can be used to make predictions.

"""

# Dictionary to store the trained model objects
trained_models = {}

if not X_train.empty and not y_train.empty:
    print("\nBuilding (training) the selected models...")
    for name, model in models.items():
        print(f"Now training: {name}")
        try:
            # Fit the model on the training data
            model.fit(X_train, y_train)
            trained_models[name] = model # Store the trained model
            print(f"Successfully trained: {name}")
        except Exception as e:
            print(f"An error occurred while training {name}: {e}")
            trained_models[name] = None # Mark as None if training failed
    print("\nModel training process complete.")
else:
    print("\nCritical Error: Training data (X_train or y_train) is empty.")
    print("Cannot build models. Please ensure Phase 3 (Data Preparation) was executed correctly using 'heart.csv'.")

"""**Assess Model(s)**

Once the models are trained, their predictive performance is assessed using the independent test set (X_test and y_test). This step is crucial for understanding how well each model generalizes to new, unseen data. For binary classification tasks like heart disease prediction, several metrics are typically used:

*   **Accuracy**: The overall proportion of correct predictions (true positives and negatives).

*   **Precision**: Of all instances predicted as positive, what fraction were positive? (TP / (TP + FP)). Essential when the cost of a false positive is high.
*   **F1-Score**: The harmonic mean of Precision and Recall, providing a single score that balances both. Useful when there's a class imbalance or when precision and recall are essential.
*   **ROC AUC (Area Under the Receiver Operating Characteristic Curve)**: This metric evaluates the model's ability to discriminate between positive and negative classes across all thresholds. An AUC of 1.0 indicates a perfect classifier, while 0.5 suggests no better than random guessing.

A detailed classification report and a confusion matrix also provide deeper insights into the types of errors the model makes.
"""

# Dictionary to store the performance metrics for each model
model_performance = {}

if not X_test.empty and not y_test.empty:
    print("\nAssessing model performance on the test set...")
    for name, model in trained_models.items():
        if model is not None: # Check if the model was trained successfully
            print(f"\nEvaluating: {name}")
            try:
                # Make predictions on the test data
                y_pred = model.predict(X_test)

                # For ROC AUC, predict_proba is needed for most classifiers
                if hasattr(model, "predict_proba"):
                    y_pred_proba = model.predict_proba(X_test)[:, 1]
                else: # For models like SVC without probability=True by default or if it's not applicable
                    # Decision function can be used, or this metric might be skipped if not directly available
                    y_pred_proba = model.decision_function(X_test)
                    # Normalize if necessary, or handle appropriately based on model
                    # For simplicity here, we rely on probability=True set for SVC.
                    # If a model truly can't output probabilities, roc_auc_score might need y_pred instead,
                    # but this is less standard.

                # Calculate metrics
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, zero_division=0)
                recall = recall_score(y_test, y_pred, zero_division=0)
                f1 = f1_score(y_test, y_pred, zero_division=0)
                roc_auc = roc_auc_score(y_test, y_pred_proba)

                # Store metrics
                model_performance[name] = {
                    "Accuracy": accuracy,
                    "Precision": precision,
                    "Recall": recall,
                    "F1-Score": f1,
                    "ROC AUC": roc_auc
                }

                # Print metrics
                print(f"  Accuracy: {accuracy:.4f}")
                print(f"  Precision: {precision:.4f}")
                print(f"  Recall: {recall:.4f}")
                print(f"  F1-Score: {f1:.4f}")
                print(f"  ROC AUC: {roc_auc:.4f}")

                print("\n  Classification Report:")
                print(classification_report(y_test, y_pred, zero_division=0))

                print("\n  Confusion Matrix:")
                print(confusion_matrix(y_test, y_pred))

            except Exception as e:
                print(f"An error occurred while assessing {name}: {e}")
                model_performance[name] = None # Mark as None if assessment failed
        else:
            print(f"\nSkipping assessment for {name} as it was not trained successfully.")

    # Creating a summary DataFrame of model performance
    if model_performance:
        performance_df = pd.DataFrame(model_performance).T # Transpose for models as rows
        print("\n\n--- Overall Model Performance Summary ---")
        # Sort by a key metric, e.g., ROC AUC or F1-Score, for easier comparison
        print(performance_df.sort_values(by="ROC AUC", ascending=False))
    else:
        print("\nNo models were successfully trained or assessed to summarize performance.")
else:
    print("\nCritical Error: Test data (X_test or y_test) is empty.")
    print("Cannot assess models. Please ensure Phase 3 (Data Preparation) was executed correctly.")

"""The performance summary table allows for a direct comparison of the models. For medical diagnosis like heart disease prediction, recall for the positive class (presence of disease) is often critical, as failing to identify a sick patient (false negative) can have severe consequences. ROC AUC provides a good overall measure of a model's discriminative ability. The "best" model will depend on the specific priorities (e.g., minimizing false negatives vs. false positives).

**Tune Model(s) (Optional but Recommended)**

Many machine learning algorithms have hyperparameters that are set before the learning process begins. The performance of a model can often be significantly improved by tuning these hyperparameters. Techniques like Grid Search CV (Cross-Validation) or Randomized Search CV systematically explore different combinations of hyperparameter values to find the optimal set. Grid Search CV exhaustively tries all specified combinations. This step, while potentially time-consuming, is highly recommended for maximizing model performance. Here, a simplified Grid Search CV is demonstrated for the Random Forest model as an example.
"""

# Optional: Hyperparameter tuning for one model (e.g., Random Forest)
# Check if Random Forest was trained and if X_train is available
if 'Random Forest' in trained_models and trained_models['Random Forest'] is not None and not X_train.empty:
    print("\n\n--- Hyperparameter Tuning (Example for Random Forest) ---")

    # Define a parameter grid for Random Forest.
    # This is a relatively small grid for demonstration; a real-world search might be more extensive.
    param_grid_rf = {
        'n_estimators': [50, 100, 200],       # Number of trees in the forest
        'max_depth': [None, 10, 20, 30],       # Maximum depth of the trees
        'min_samples_split': [2, 5, 10],     # Minimum number of samples required to split an internal node
        'min_samples_leaf': [1, 2, 4]        # Minimum number of samples required to be at a leaf node
    }

    # Initialize GridSearchCV
    # cv=3 means 3-fold cross-validation. Using a small number for speed in this example.
    # Common practice is cv=5 or cv=10.
    # Scoring can be 'roc_auc', 'accuracy', 'f1', 'recall', 'precision', etc.
    grid_search_rf = GridSearchCV(
        estimator=RandomForestClassifier(random_state=42), # Base model
        param_grid=param_grid_rf,                          # Parameters to search
        cv=3,                                              # Number of cross-validation folds
        scoring='roc_auc',                                 # Metric to optimize
        verbose=1,                                         # Output progress messages
        n_jobs=-1                                          # Use all available CPU cores
    )

    print("Starting GridSearchCV for Random Forest. This may take some time...")
    try:
        # Fit GridSearchCV to the training data
        grid_search_rf.fit(X_train, y_train)

        print("\nBest hyperparameters found for Random Forest by GridSearchCV:")
        print(grid_search_rf.best_params_)

        # Get the best model found by GridSearchCV
        best_rf_model = grid_search_rf.best_estimator_

        # Add this tuned model to our trained_models dictionary for consistent evaluation
        trained_models['Tuned Random Forest'] = best_rf_model

        # Evaluate the tuned model (similar to step 3 for this new model)
        print("\nAssessing the Tuned Random Forest Model:")
        y_pred_tuned_rf = best_rf_model.predict(X_test)
        y_pred_proba_tuned_rf = best_rf_model.predict_proba(X_test)[:, 1]

        accuracy_tuned = accuracy_score(y_test, y_pred_tuned_rf)
        precision_tuned = precision_score(y_test, y_pred_tuned_rf, zero_division=0)
        recall_tuned = recall_score(y_test, y_pred_tuned_rf, zero_division=0)
        f1_tuned = f1_score(y_test, y_pred_tuned_rf, zero_division=0)
        roc_auc_tuned = roc_auc_score(y_test, y_pred_proba_tuned_rf)

        tuned_rf_metrics = {
            "Accuracy": accuracy_tuned,
            "Precision": precision_tuned,
            "Recall": recall_tuned,
            "F1-Score": f1_tuned,
            "ROC AUC": roc_auc_tuned
        }

        print(f"  Tuned Accuracy: {accuracy_tuned:.4f}")
        print(f"  Tuned Precision: {precision_tuned:.4f}")
        print(f"  Tuned Recall: {recall_tuned:.4f}")
        print(f"  Tuned F1-Score: {f1_tuned:.4f}")
        print(f"  Tuned ROC AUC: {roc_auc_tuned:.4f}")
        print("\n  Tuned Classification Report:")
        print(classification_report(y_test, y_pred_tuned_rf, zero_division=0))
        print("\n  Tuned Confusion Matrix:")
        print(confusion_matrix(y_test, y_pred_tuned_rf))

        # Update the performance DataFrame with the tuned model's results
        if 'performance_df' in locals() and isinstance(performance_df, pd.DataFrame):
            performance_df.loc['Tuned Random Forest'] = tuned_rf_metrics
            print("\n--- Updated Model Performance Summary (including tuned model) ---")
            print(performance_df.sort_values(by="ROC AUC", ascending=False))
        else: # If performance_df wasn't created due to prior errors or only tuned model exists
            tuned_performance_df = pd.DataFrame({'Tuned Random Forest': tuned_rf_metrics}).T
            print("\n--- Tuned Random Forest Performance ---")
            print(tuned_performance_df)
            performance_df = tuned_performance_df # Assign for next step

    except Exception as e:
        print(f"An error occurred during GridSearchCV for Random Forest: {e}")
else:
    if X_train.empty:
        print("\nSkipping hyperparameter tuning as X_train data is not available.")
    elif 'Random Forest' not in trained_models or trained_models.get('Random Forest') is None:
        print("\nSkipping hyperparameter tuning for Random Forest as the base model was not trained successfully.")

"""**Select Best Model**

The final step in the modeling phase is to select the best model from the ones built and evaluated (including any tuned versions). The "best" model is chosen based on its performance on the test set, considering the project's specific objectives and the relative importance of different evaluation metrics. For example, in medical diagnosis, recall and ROC AUC are often prioritized. Business constraints like model interpretability or deployment complexity might also influence the decision.
"""

print("\n\n--- Selecting Best Model ---")

if 'performance_df' in locals() and isinstance(performance_df, pd.DataFrame) and not performance_df.empty:
    # Sort models by a primary metric, e.g., ROC AUC or F1-Score
    # For medical diagnosis, ROC AUC is a good general measure,
    # and F1-score is good if there's an imbalance or if both precision and recall are important.
    # Recall for the positive class (detecting disease) is often paramount.

    # Let's assume ROC AUC is the primary metric for selection in this example.
    best_model_name = performance_df['ROC AUC'].idxmax()
    best_model_metrics = performance_df.loc[best_model_name]

    print("Based on the ROC AUC score, the best performing model is:")
    print(f"Model: {best_model_name}")
    print("With the following metrics:")
    print(best_model_metrics)

    # Further considerations:
    print("\nFurther Considerations for Model Selection:")
    print("- If minimizing false negatives (e.g., failing to detect heart disease) is critical, prioritize models with higher Recall for the positive class.")
    print("- If minimizing false positives (e.g., wrongly diagnosing heart disease) is critical, prioritize models with higher Precision for the positive class.")
    print("- F1-Score offers a balance between Precision and Recall.")
    print("- Interpretability: Logistic Regression and Decision Trees are generally more interpretable than Random Forests or SVCs.")
    print("- The final choice should align with the specific goals and constraints of the project.")

    # Example: Retrieve the actual best model object
    if 'Tuned Random Forest' in best_model_name and 'grid_search_rf' in locals():
        final_selected_model = grid_search_rf.best_estimator_
    elif best_model_name in trained_models:
        final_selected_model = trained_models[best_model_name]
    else:
        final_selected_model = None
    print(f"\nThe selected model object: {final_selected_model}")

elif 'tuned_performance_df' in locals() and isinstance(tuned_performance_df, pd.DataFrame) and not tuned_performance_df.empty:
    # This case handles if only the tuned model's performance was available (e.g., if base models failed)
    best_model_name = tuned_performance_df['ROC AUC'].idxmax()
    best_model_metrics = tuned_performance_df.loc[best_model_name]
    print("Based on the ROC AUC score, the best performing model (from available tuned models) is:")
    print(f"Model: {best_model_name}")
    print("With the following metrics:")
    print(best_model_metrics)
else:
    print("Model performance data is not available. Cannot select the best model.")
    print("Please ensure prior steps (Build and Assess Models) were completed successfully.")

"""This concludes Phase 4: Modeling. The output of this phase is typically a selected model (or a few candidate models) that performs best according to the defined criteria, ready for further evaluation and eventual deployment (Phase 5 and 6 of CRISP-DM).

# **Phase 5: Evaluation**

This phase critically appraises the model and the project journey, ensuring the outcomes align with the initial goals and determining the path forward.

## **Evaluate Results**
"""

# ---
# Heart Disease Prediction UI (Standalone Section)
st.markdown('---')
st.header('Heart Disease Prediction Tool')

import pickle
import joblib

# --- 1. Load Model and Preprocessing Objects ---
try:
    model_pred = joblib.load('preprocessor.pkl')
    scaler_pred = joblib.load('scaler.pkl')
    model_columns_pred = joblib.load('columns_model.pkl') 
except FileNotFoundError:
    st.error("Model or preprocessing files not found. Please ensure 'preprocessor.pkl', 'scaler.pkl', and 'columns_model.pkl' are present.")
    class DummyModel:
        def predict(self, X): return np.array([0])
        def predict_proba(self, X): return np.array([[1.0, 0.0]])
    class DummyScaler:
        def transform(self, X): return X
    model_pred = DummyModel()
    scaler_pred = DummyScaler()
    model_columns_pred = None
    st.warning("Using dummy model and scaler as actual files were not found. Predictions will not be accurate.")
except Exception as e:
    st.error(f"An error occurred loading files: {e}")
    model_pred = None

st.sidebar.header("Patient Input Features (Prediction Tool)")

def user_input_features_pred():
    age = st.sidebar.number_input("Age (years)", min_value=1, max_value=120, value=50, step=1, key='age_pred')
    sex_options = {"Male": 1, "Female": 0}
    sex_label = st.sidebar.selectbox("Sex", options=list(sex_options.keys()), index=0, key='sex_pred')
    sex = sex_options[sex_label]
    cp_options = {"Typical Angina": 0, "Atypical Angina": 1, "Non-anginal Pain": 2, "Asymptomatic": 3}
    cp_label = st.sidebar.selectbox("Chest Pain Type (cp)", options=list(cp_options.keys()), index=2, key='cp_pred')
    cp = cp_options[cp_label]
    trestbps = st.sidebar.number_input("Resting Blood Pressure (trestbps, mm Hg)", min_value=50, max_value=250, value=120, step=1, key='trestbps_pred')
    chol = st.sidebar.number_input("Serum Cholesterol (chol, mg/dl)", min_value=100, max_value=600, value=200, step=1, key='chol_pred')
    fbs_options = {"True (> 120 mg/dl)": 1, "False (<= 120 mg/dl)": 0}
    fbs_label = st.sidebar.selectbox("Fasting Blood Sugar > 120 mg/dl (fbs)", options=list(fbs_options.keys()), index=1, key='fbs_pred')
    fbs = fbs_options[fbs_label]
    restecg_options = {"Normal": 0, "ST-T Wave Abnormality": 1, "Left Ventricular Hypertrophy": 2}
    restecg_label = st.sidebar.selectbox("Resting ECG (restecg)", options=list(restecg_options.keys()), index=0, key='restecg_pred')
    restecg = restecg_options[restecg_label]
    thalach = st.sidebar.number_input("Maximum Heart Rate Achieved (thalach)", min_value=50, max_value=220, value=150, step=1, key='thalach_pred')
    exang_options = {"Yes": 1, "No": 0}
    exang_label = st.sidebar.selectbox("Exercise Induced Angina (exang)", options=list(exang_options.keys()), index=1, key='exang_pred')
    exang = exang_options[exang_label]
    oldpeak = st.sidebar.number_input("ST Depression Induced by Exercise (oldpeak)", min_value=0.0, max_value=10.0, value=1.0, step=0.1, format="%.1f", key='oldpeak_pred')
    slope_options = {"Upsloping": 0, "Flat": 1, "Downsloping": 2}
    slope_label = st.sidebar.selectbox("Slope of Peak Exercise ST Segment (slope)", options=list(slope_options.keys()), index=1, key='slope_pred')
    slope = slope_options[slope_label]
    ca = st.sidebar.number_input("Number of Major Vessels Colored by Flouroscopy (ca)", min_value=0, max_value=4, value=0, step=1, key='ca_pred')
    thal_options = {"Normal": 1, "Fixed Defect": 2, "Reversible Defect": 3}
    thal_label = st.sidebar.selectbox("Thalassemia (thal)", options=list(thal_options.keys()), index=1, key='thal_pred')
    thal = thal_options[thal_label]
    data = {
        'age': age, 'sex': sex, 'cp': cp, 'trestbps': trestbps, 'chol': chol, 'fbs': fbs, 'restecg': restecg,
        'thalach': thalach, 'exang': exang, 'oldpeak': oldpeak, 'slope': slope, 'ca': ca, 'thal': thal
    }
    features_df = pd.DataFrame(data, index=[0])
    return features_df

input_df_pred = user_input_features_pred()
st.subheader("Patient Input Parameters (Prediction Tool):")
st.write(input_df_pred)

if model_pred is not None and st.button("Predict Heart Disease Likelihood", key='predict_btn_pred'):
    try:
        numerical_features_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
        processed_df = input_df_pred.copy()
        if scaler_pred is not None and all(col in processed_df.columns for col in numerical_features_to_scale):
            processed_df[numerical_features_to_scale] = scaler_pred.transform(processed_df[numerical_features_to_scale])
            st.write("Numerical features scaled.")
        else:
            st.warning("Scaler not available or some numerical features missing from input for scaling. Using unscaled numerical features.")
        categorical_features_to_encode = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
        processed_df = pd.get_dummies(processed_df, columns=categorical_features_to_encode, drop_first=True)
        st.write("Categorical features encoded.")
        if model_columns_pred is not None:
            current_cols = processed_df.columns
            for col in model_columns_pred:
                if col not in current_cols:
                    processed_df[col] = 0
            processed_df = processed_df[model_columns_pred]
        else:
            st.warning("Model training columns list ('model_columns.pkl') not loaded. Categorical encoding might be misaligned.")
        st.write("Preprocessed data for prediction (first row shown):")
        st.write(processed_df.head())
        prediction = model_pred.predict(processed_df)
        prediction_proba = model_pred.predict_proba(processed_df)
        st.subheader("Prediction:")
        if prediction[0] == 1:
            st.error("Heart Disease Likely")
        else:
            st.success("No Heart Disease Likely")
        st.subheader("Prediction Probability:")
        st.write(f"Probability of No Heart Disease: {prediction_proba[0][0]:.2f}")
        st.write(f"Probability of Heart Disease: {prediction_proba[0][1]:.2f}")
    except AttributeError as ae:
        if 'predict_proba' in str(ae) and 'DummyModel' in str(type(model_pred)):
            st.warning("Dummy model does not have 'predict_proba'. Only class prediction shown.")
            prediction = model_pred.predict(processed_df)
            st.subheader("Prediction:")
            if prediction[0] == 1:
                st.error("Heart Disease Likely (Dummy Prediction)")
            else:
                st.success("No Heart Disease Likely (Dummy Prediction)")
        else:
            st.error(f"An error occurred during prediction: {ae}. Check if the model and preprocessors are loaded correctly and if data format is as expected.")
    except Exception as e:
        st.error(f"An unexpected error occurred during prediction: {e}")
        st.error("Please check the input values and ensure the model/preprocessing files are correctly loaded and configured.")

st.sidebar.markdown("---")
st.sidebar.subheader("Model Insights (Example)")
# You would load/calculate these from your Phase 4 work
# For example:
# st.sidebar.write("Model Accuracy: 85%") # Replace with your actual accuracy
# st.sidebar.write("Key Predictors: Chest Pain Type, Max Heart Rate, Age")
st.markdown("---")
st.markdown("Developed as part of a data science project.")
st.markdown("Remember to consult with a healthcare professional for any medical concerns.")